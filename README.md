# 多模态生成
## 项目背景：
在医疗领域，准确的临床推理和诊断对于患者的治疗至关重要。然而，现有的语言模型在处理复杂的医疗问题时可能存在不足。为了提升模型在医疗领域的表现，本项目利用Unsloth框架对DeepSeek-R1-Distill-Llama-8B模型进行微调，使其能够更好地理解和生成与医疗相关的文本内容。
## 项目目标：
使用Unsloth框架对DeepSeek-R1-Distill-Llama-8B模型进行微调，以提高其在医疗领域问题回答任务中的准确性和逻辑性。
通过微调后的模型，生成高质量的医疗领域文本内容，为医疗专家提供辅助决策支持。
将微调后的模型部署到Hugging Face Hub，方便其他研究人员和开发者使用。
## 主要工作内容：
环境搭建与模型加载：
使用pip安装Unsloth库，并从GitHub获取最新的Unsloth代码。
配置Hugging Face和WandB的认证信息，以便使用相关功能。
加载DeepSeek-R1-Distill-Llama-8B模型，并设置最大序列长度为2048，以支持长文本输入。
## 模型预训练性能测试：
设计一个医疗领域的问题模板，用于测试模型在微调前的性能。
使用模型生成回答，并观察其逻辑性和准确性。
## 数据集加载与处理：
从Hugging Face Hub加载医疗领域推理数据集（FreedomIntelligence/medical-o1-reasoning-SFT）。
对数据集进行预处理，将其转换为适合模型训练的格式。
## 模型微调：
使用Unsloth的FastLanguageModel对模型进行微调，添加LoRA权重以提高模型的适应性。
配置训练参数，包括批量大小、梯度累积步数、学习率等。
使用SFTTrainer进行模型训练，并通过WandB可视化训练过程。
## 模型性能评估：
在微调完成后，使用新的医疗问题测试模型的性能。
比较微调前后模型的回答质量，评估微调的效果。
模型保存与部署：
将微调后的模型保存到本地，并将其推送到Hugging Face Hub。
提供模型的使用说明，方便其他用户下载和使用。
